{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.18","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":14242,"databundleVersionId":568274,"sourceType":"competition"}],"dockerImageVersionId":31091,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nRAM-lean DBSCAN Fraud Detection (IEEE-CIS)\n- Subsampled training\n- Float32 downcast\n- Drop ultra-sparse columns\n- PCA on subset\n- Predict by distance to DBSCAN core samples\n\nRequires: numpy, pandas, scikit-learn\n\"\"\"\n\nimport os, gc, warnings, numpy as np, pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, auc as sk_auc\n\nwarnings.filterwarnings(\"ignore\")\n\n# ---------------- Config (tune these if RAM is tight) ----------------\nRANDOM_STATE        = 42\nTEST_SIZE           = 0.2\nMAX_TRAIN_SAMPLES   = 150_000     # reduce to 75_000 or 50_000 if needed\nPCA_COMPONENTS      = 20          # 10â€“30 is usually fine\nMISSING_DROP_THRESH = 0.90        # drop cols with >90% missing\nMIN_SAMPLES_GRID    = [15, 30]\nPERCENTILES_GRID    = [95, 97.5, 99]  # for eps from k-distance\nK_FOR_KDIST         = 30          # usually ~= min_samples\nDATA_DIR            = \"/kaggle/input/ieee-fraud-detection\"\nTXN_FILE, ID_FILE   = \"train_transaction.csv\", \"train_identity.csv\"\n\n# ---------------- Utils ----------------\ndef load_df():\n    tx = pd.read_csv(os.path.join(DATA_DIR, TXN_FILE), index_col=\"TransactionID\")\n    idf = pd.read_csv(os.path.join(DATA_DIR, ID_FILE), index_col=\"TransactionID\")\n    df = tx.join(idf, how=\"left\")\n    if \"TransactionDT\" in df.columns:\n        df = df.sort_values(\"TransactionDT\")\n    if \"isFraud\" not in df.columns:\n        raise KeyError(\"Missing 'isFraud'\")\n    return df\n\ndef drop_sparse_and_keep_numeric(df):\n    # Drop columns too sparse (saves RAM immediately)\n    miss_frac = df.isna().mean()\n    keep_cols = miss_frac[miss_frac <= MISSING_DROP_THRESH].index\n    df = df[keep_cols]\n    # Keep numeric only for DBSCAN\n    y = df[\"isFraud\"].astype(np.int8)\n    X = df.drop(columns=[\"isFraud\"]).select_dtypes(include=[np.number])\n    # Downcast to float32/int32\n    for c in X.columns:\n        if pd.api.types.is_float_dtype(X[c]):\n            X[c] = X[c].astype(np.float32)\n        elif pd.api.types.is_integer_dtype(X[c]):\n            # small ints are fine; cast to int32\n            X[c] = X[c].astype(np.int32)\n    return X, y\n\ndef chrono_split(X, y, test_size=TEST_SIZE):\n    n = len(X)\n    split = int((1 - test_size) * n)\n    return X.iloc[:split], X.iloc[split:], y.iloc[:split], y.iloc[split:]\n\ndef stratified_subset(Xtr, ytr, max_samples=MAX_TRAIN_SAMPLES, random_state=RANDOM_STATE):\n    \"\"\"Keep ALL frauds + random nonfrauds to cap total size.\"\"\"\n    fraud_idx = ytr[ytr == 1].index\n    non_idx   = ytr[ytr == 0].index\n    keep_f = fraud_idx\n    remaining = max_samples - len(keep_f)\n    if remaining <= 0:\n        # if extreme: just use all frauds (rare) and sample some of them\n        keep_f = np.random.RandomState(random_state).choice(fraud_idx, size=max_samples, replace=False)\n        keep = np.array(keep_f)\n    else:\n        rng = np.random.RandomState(random_state)\n        take_non = min(remaining, len(non_idx))\n        keep_non = rng.choice(non_idx, size=take_non, replace=False)\n        keep = np.concatenate([keep_f, keep_non])\n    keep = pd.Index(keep)\n    Xsub, ysub = Xtr.loc[keep].copy(), ytr.loc[keep].copy()\n    return Xsub, ysub\n\ndef fit_scaler_pca(Xref, n_components=PCA_COMPONENTS):\n    med = Xref.median()          # robust + cheap\n    Xf  = Xref.fillna(med)\n    scaler = StandardScaler()\n    Xs = scaler.fit_transform(Xf)\n    n_comp = min(n_components, Xs.shape[1])\n    pca = PCA(n_components=n_comp, svd_solver=\"randomized\", random_state=RANDOM_STATE)\n    Xp = pca.fit_transform(Xs).astype(np.float32)\n    return med, scaler, pca, Xp\n\ndef transform_with(med, scaler, pca, X):\n    Xf = X.fillna(med)\n    Xs = scaler.transform(Xf)\n    return pca.transform(Xs).astype(np.float32)\n\ndef kdistances(X, k=K_FOR_KDIST):\n    # Use Ball Tree to keep memory down\n    nn = NearestNeighbors(n_neighbors=k, algorithm=\"ball_tree\", n_jobs=-1)\n    nn.fit(X)\n    dists, _ = nn.kneighbors(X)\n    return dists[:, -1].astype(np.float32)\n\ndef fit_dbscan(X, eps, min_samples):\n    # 'ball_tree' helps in mid/high dims better than kd_tree\n    return DBSCAN(eps=eps, min_samples=min_samples, algorithm=\"ball_tree\", n_jobs=-1).fit(X)\n\ndef core_nn(db, X_fit):\n    core_idx = db.core_sample_indices_\n    if core_idx is None or len(core_idx) == 0:\n        return None, None\n    core_pts = X_fit[core_idx]\n    nn = NearestNeighbors(n_neighbors=1, algorithm=\"ball_tree\", n_jobs=-1).fit(core_pts)\n    return nn, core_pts\n\ndef predict_scores(nn_core, eps, X_new):\n    if nn_core is None:\n        # No cores -> everything anomalous (fallback)\n        return np.ones(len(X_new), dtype=np.int8), np.ones(len(X_new), dtype=np.float32)\n    d, _ = nn_core.kneighbors(X_new, n_neighbors=1)\n    d = d.ravel().astype(np.float32)\n    yhat = (d > eps).astype(np.int8)   # > eps => outside density -> \"fraud\"\n    return yhat, d\n\n# ---------------- Main ----------------\nif __name__ == \"__main__\":\n    df = load_df()\n    print(f\"Merged: {df.shape}\")\n\n    # Keep numeric, drop ultra-sparse, downcast\n    X, y = drop_sparse_and_keep_numeric(df)\n    del df; gc.collect()\n    print(f\"After filtering -> X: {X.shape}, y: {y.shape} | numeric cols={X.shape[1]}\")\n\n    # Chronological split (avoids leakage)\n    Xtr, Xte, ytr, yte = chrono_split(X, y)\n    del X, y; gc.collect()\n    print(f\"Train: {Xtr.shape}, Test: {Xte.shape}, Frauds in train: {ytr.sum()}\")\n\n    # Subsample train to fit transforms + DBSCAN\n    Xsub, ysub = stratified_subset(Xtr, ytr, MAX_TRAIN_SAMPLES)\n    print(f\"Subset for fitting: {Xsub.shape} (kept all frauds: {ysub.sum()})\")\n\n    # Fit scaler + PCA on subset (RAM-friendly), no need to transform full train\n    med, scaler, pca, Xsub_pca = fit_scaler_pca(Xsub, PCA_COMPONENTS)\n    print(f\"PCA dims: {Xsub_pca.shape[1]}\")\n\n    # Choose eps from k-distance percentiles on subset\n    kd = kdistances(Xsub_pca, k=K_FOR_KDIST)\n    best = {\"ap\": -np.inf, \"eps\": None, \"min_samples\": None}\n    for ms in MIN_SAMPLES_GRID:\n        for p in PERCENTILES_GRID:\n            eps = np.percentile(kd, p).item()\n            db  = fit_dbscan(Xsub_pca, eps=eps, min_samples=ms)\n            nnc, _ = core_nn(db, Xsub_pca)\n            # Use subset as a quick proxy for AP (unsupervised; labels only to guide tuning)\n            _, scores = predict_scores(nnc, eps, Xsub_pca)\n            ap = average_precision_score(ysub, scores)\n            if ap > best[\"ap\"]:\n                best.update({\"ap\": ap, \"eps\": eps, \"min_samples\": ms})\n    eps_opt, ms_opt = best[\"eps\"], best[\"min_samples\"]\n    print(f\"Chosen: eps={eps_opt:.5f}, min_samples={ms_opt} (subset AP={best['ap']:.5f})\")\n\n    # Fit final DBSCAN on subset\n    db  = fit_dbscan(Xsub_pca, eps=eps_opt, min_samples=ms_opt)\n    nnc, core_pts = core_nn(db, Xsub_pca)\n\n    # Transform ONLY the test set and score\n    Xte_pca = transform_with(med, scaler, pca, Xte)\n    yhat, scores = predict_scores(nnc, eps_opt, Xte_pca)\n\n    # Metrics (scores: higher = more anomalous)\n    roc  = roc_auc_score(yte, scores)\n    ap   = average_precision_score(yte, scores)\n    prec, rec, _ = precision_recall_curve(yte, scores)\n    pr_auc = sk_auc(rec, prec)\n\n    print(\"\\n=== DBSCAN (RAM-lean) Test Metrics ===\")\n    print(f\"ROC-AUC: {roc:.5f}\")\n    print(f\"Average Precision (AP): {ap:.5f}\")\n    print(f\"PR-AUC: {pr_auc:.5f}\")\n    print(f\"Predicted frauds (noise): {int(yhat.sum())} / {len(yhat)}\")\n\n    # Save outputs (optional)\n    out = pd.DataFrame({\"score\": scores, \"pred_fraud\": yhat, \"isFraud\": yte.values}, index=Xte.index)\n    out.to_csv(\"dbscan_ram_lean_predictions.csv\")\n    print(\"Saved: dbscan_ram_lean_predictions.csv\")\n\n    # Cleanup\n    del Xtr, Xte, ytr, yte, Xsub, ysub, Xsub_pca, Xte_pca, kd, scores, yhat\n    gc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T11:23:43.530187Z","iopub.execute_input":"2025-08-18T11:23:43.530506Z","iopub.status.idle":"2025-08-18T12:51:09.406812Z","shell.execute_reply.started":"2025-08-18T11:23:43.530479Z","shell.execute_reply":"2025-08-18T12:51:09.400031Z"}},"outputs":[{"name":"stdout","text":"Merged: (590540, 433)\nAfter filtering -> X: (590540, 391), y: (590540,) | numeric cols=391\nTrain: (472432, 391), Test: (118108, 391), Frauds in train: 16599\nSubset for fitting: (150000, 391) (kept all frauds: 16599)\nPCA dims: 20\nChosen: eps=7.44825, min_samples=30 (subset AP=0.19669)\n\n=== DBSCAN (RAM-lean) Test Metrics ===\nROC-AUC: 0.72272\nAverage Precision (AP): 0.09268\nPR-AUC: 0.09263\nPredicted frauds (noise): 4104 / 118108\nSaved: dbscan_ram_lean_predictions.csv\n","output_type":"stream"}],"execution_count":1}]}